
1. The `input` could contain Tools that LLM could use from MCP or other sources. An implication of this would be that model should be capable of Tools use - Its important that Model Card has valid models and model capabilities for file, image, tools, audio should be validated before accepting model for the model card

2. The `output` will have responses to tool usage, and usage statistics

3. The `storage` should be adaptable to a local db at a future date

4. Add `vite` in the tech stack

5. Integrate with LLM  Service : Restrict it to `Openrouter` the cloud LLM Service and `Ollama` for local models

